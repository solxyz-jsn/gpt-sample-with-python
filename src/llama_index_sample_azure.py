# -*- coding: utf-8 -*-

"""LlamaIndexã‚µãƒ³ãƒ—ãƒ«

LlamaIndexã‚’ä½¿ç”¨ã—ã¦ã€å¤–éƒ¨ã®ãƒ†ã‚­ã‚¹ãƒˆã®æƒ…å ±ã‚’å‚ç…§ã—ã¦
å›ç­”ã‚’ã•ã›ã‚‹ã‚µãƒ³ãƒ—ãƒ«

"""

import streamlit as st
from langchain.embeddings import OpenAIEmbeddings
from langchain.chat_models import AzureChatOpenAI
from llama_index import (
    GPTVectorStoreIndex,
    SimpleDirectoryReader,
    ServiceContext,
    LLMPredictor,
    StorageContext,
    load_index_from_storage,
    LangchainEmbedding,
    set_global_service_context,
)
import logging
import sys
import os
from dotenv import load_dotenv
from azure.identity import DefaultAzureCredential

# ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
load_dotenv()

# ãƒ­ã‚°ã®è¨­å®š
logging.basicConfig(stream=sys.stdout, level=logging.DEBUG, force=True)


def create_index(dir_path: str) -> GPTVectorStoreIndex:
    """
    å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã€GPTVectorStoreIndexã‚’ä½œæˆã™ã‚‹ã€‚

    Parameters
    ----------
    dir_path : str
        å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹ã€‚

    Returns
    -------
    GPTVectorStoreIndex
        ä½œæˆã•ã‚ŒãŸGPTVectorStoreIndexã€‚

    """
    # å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
    documents = SimpleDirectoryReader(os.path.join(dir_path, "data")).load_data()

    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ä½œæˆ
    index = GPTVectorStoreIndex.from_documents(documents)

    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ä¿å­˜
    index.storage_context.persist()

    return index


logging.basicConfig(stream=sys.stdout, level=logging.DEBUG, force=True)
default_credential = DefaultAzureCredential()
token = default_credential.get_token("https://cognitiveservices.azure.com/.default")
# ChatGPT-3.5ã®ãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®ä½œæˆ
api_key: str = token.token
api_base: str = os.getenv("OPENAI_API_BASE")
api_version: str = os.getenv("OPENAI_API_VERSION")
api_type: str = os.getenv("OPENAI_API_TYPE")
ai_model: str = os.getenv("AZURE_MODEL")
embedding_model: str = os.getenv("AZURE_EMBEDDING_MODEL")

# OpenAIã®ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ
llm = AzureChatOpenAI(
    openai_api_type=api_type,
    openai_api_base=api_base,
    openai_api_key=api_key,
    openai_api_version=api_version,
    deployment_name=ai_model,
)

# Embeddingãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ
embedding_llm = LangchainEmbedding(
    OpenAIEmbeddings(
        deployment=embedding_model,
        openai_api_key=api_key,
        openai_api_base=api_base,
        openai_api_type=api_type,
        openai_api_version=api_version,
    ),
    embed_batch_size=16,
)

# å®Ÿè¡Œãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
dir_path = os.path.dirname(os.path.realpath(__file__))

# ã‚µãƒ¼ãƒ“ã‚¹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®ä½œæˆ
service_context = ServiceContext.from_defaults(
    llm=llm,
    embed_model=embedding_llm,
)

set_global_service_context(service_context)

# ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ä½œæˆï¼ˆåˆå›ã®ã¿ï¼‰
index = create_index(dir_path)

# ä¿å­˜æ¸ˆã¿ã®ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ï¼ˆäºŒå›ç›®ä»¥é™ï¼‰
storage_context = StorageContext.from_defaults(persist_dir="./storage")
index = load_index_from_storage(storage_context)

query_engine = index.as_query_engine()

# ã‚¿ã‚¤ãƒˆãƒ«ã®ä½œæˆ
st.title("ğŸŸLlamaIndexã®ã‚µãƒ³ãƒ—ãƒ«ğŸŸ")
st.text("å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™ã€‚")

# ã‚¤ãƒ³ãƒ—ãƒƒãƒˆç”¨ã®ãƒ†ã‚­ã‚¹ãƒˆãƒœãƒƒã‚¯ã‚¹ã®ä½œæˆ
prompt = st.text_input("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")

# å…¥åŠ›ãŒã‚ã£ãŸã‚‰OpenAIã®APIã‚’å®Ÿè¡Œ
if st.button("å•ã„åˆã‚ã›é–‹å§‹"):
    try:
        response = query_engine.query(
            f"ã‚ãªãŸã¯åºƒå ±ã§ã™ã€‚å¯èƒ½ãªé™ã‚Šè©³ã—ãæ—¥æœ¬èªã§ç­”ãˆã¦ãã ã•ã„ã€‚ã¾ãŸæ–‡ç« ã«è¨˜è¼‰ã®ãªã„å ´åˆã¯ãã®æ—¨ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚ ```{prompt}```"
        )
    except Exception as e:
        print("error")
        response = str(e)
    # OpenAIã®å›ç­”ã‚’è¡¨ç¤º
    st.write(response)

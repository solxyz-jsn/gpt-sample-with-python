'''
LlamaIndexã‚’ä½¿ç”¨ã—ã¦ã€å¤–éƒ¨ã®ãƒ†ã‚­ã‚¹ãƒˆã®æƒ…å ±ã‚’å‚ç…§ã—ã¦
å›ç­”ã‚’ã•ã›ã‚‹ã‚µãƒ³ãƒ—ãƒ«
'''

import streamlit as st
from langchain.chat_models import ChatOpenAI
from llama_index import GPTVectorStoreIndex, SimpleDirectoryReader, ServiceContext, LLMPredictor, StorageContext, load_index_from_storage
import openai
import logging
import sys
import os
from dotenv import load_dotenv

# ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
load_dotenv()

# Open AI APIã‚­ãƒ¼ã®è¨­å®š
openai.api_key = os.environ["OPENAI_API_KEY"]

# ãƒ­ã‚°ã®è¨­å®š
logging.basicConfig(stream=sys.stdout, level=logging.DEBUG, force=True)

# ã‚µãƒ¼ãƒ“ã‚¹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®ä½œæˆ
service_context = ServiceContext.from_defaults(
    llm_predictor = LLMPredictor(
        llm=ChatOpenAI(model_name="gpt-3.5-turbo")
        )
    )

# å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
documents = SimpleDirectoryReader('./src/data').load_data()
## ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ä½œæˆï¼ˆï¼Šåˆå›ã®ã¿å®Ÿè¡Œï¼‰
# index = GPTVectorStoreIndex.from_documents(documents,service_context=service_context)
## ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ä¿å­˜ï¼ˆï¼Šåˆå›ã®ã¿å®Ÿè¡Œï¼‰
# index.storage_context.persist()

# ä¿å­˜æ¸ˆã¿ã®ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
storage_context = StorageContext.from_defaults(persist_dir='./storage')
index = load_index_from_storage(storage_context)
query_engine = index.as_query_engine()

# ã‚¿ã‚¤ãƒˆãƒ«ã®ä½œæˆ
st.title('ğŸŸllamaindexã®ã‚µãƒ³ãƒ—ãƒ«ğŸŸ')
st.text('ã‚½ãƒ«ã‚¯ã‚·ãƒ¼ã‚ºã®æ²¿é©ã‚’èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™ã€‚')

# ã‚¤ãƒ³ãƒ—ãƒƒãƒˆç”¨ã®ãƒ†ã‚­ã‚¹ãƒˆãƒœãƒƒã‚¯ã‚¹ã®ä½œæˆ
prompt = st.text_input('è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚')

# å…¥åŠ›ãŒã‚ã£ãŸã‚‰OpenAIã®APIã‚’å®Ÿè¡Œ
if prompt:
    try:
        response = query_engine.query(prompt)
    except Exception as e:
        response = str(e)
    # OpenAIã®å›ç­”ã‚’è¡¨ç¤º
    st.write(response.response)

